{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                Reflections\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "General Overview:\n",
    "\n",
    "The direction I chose to explore for the pipeline was to select reasonable parameters for the Gaussian blur, Canny edge detection and Hough line voting procedures.  These parameters were similar to those that were effective for the practice problems, but a bit different for the specific images and videos presented in this problem set.  \n",
    "\n",
    "Once these preliminaries were performed, the resulting image was masked to a bounding isosceles trapezoid that contains the lanes.  This ended up being proportional of the image size, as the position of the camera in the car relative to the lanes is reasonably invariant.  Doing it this way availed using the same routine for the two test videos as well as the larger challenge video.  \n",
    "\n",
    "The horizon is set closer than the actual horizon.  The reason for this is that there is a lot of contrast on the horizon, and this contributes to noise that interferes with lane line detection.  This issue proved to be a balance in terms of the minimum line length, where if it is too small there is road noise, if it is too large then the broken road segments are harder to detect.  Similarly with the kernel size for the Gaussian blur, if it is too low then road noise contributes excessively, but if it is too large then features can be missed more easily.\n",
    "\n",
    "For the left and right lane lines separately I observe if the slope for each line returned from the above meets a reasonably criteria.  If so then it is a member of a pool of lines for that side that does two things:  it is used to compute the mean slope for the side, and it is used to determine the nearest point to the camera.  The reason for this choice is that it seemed like a reasonable rubric for proper operation -- the farther out points are less important since they are farther away.  I looked at lines using both the nearest and farthest as endpoints, but in practice this did not work as well, at least in my opinion.\n",
    "\n",
    "The slope and the nearest point are treated as a simple moving average filter using a specified number of points.  This smooths the jumpiness that otherwise is present.  Also, if a slope matching the requisite conditions is not detected, it simply recycles the last moving average.  This presents much better general performance, but it does come at the expense of more difficulty getting started.  Reason for this is satisfying the initial conditions for the first few frames.  The right way to do that is to have a sequence of stored values that are pre-initialized to some sane value.  What I chose to do was ignore the first frames vis-a-vis redoing the invocation to the pipeline and skipping writing out the first few.  The goal was steady state performance at the expense of how it starts. There are two video writing routines in the code, one that truncates the first few frames and one that does not.\n",
    "\n",
    "\n",
    "\n",
    "What it does not do well / how to improve:\n",
    "\n",
    "It really should have a better solution for the moving average filter.  While getting to steady state with a MA filter is always troubling, chopping frames off is probably not really the right solution.  It is a little disingenuous, I suppose.  \n",
    "\n",
    "The method should be more restrictive in terms of color.  Not just RGB color, probably something more along the lines of hue banding.  This would be helpful in being able to reduce the impact of low contrast areas like the concrete section of lane in the challenge, and reduce the effect of things like trees and their shadows.  Those issues seem to make most of the problem in the challenge, and much of the solution involves proper color pre-processing.\n",
    "\n",
    "Another problem is the relative performance of the solid line versus the dashed line.  The dashed line has less line samples and more variability, and consequently does not track as well.  I am not sure that the two can be made to perform the same, as there is no real solution to the underlying difference between the two, but it is something that can be improved to track the dashed line better.  \n",
    "\n",
    "It would be better if there was a step performed initially that could help identify the active area for the lane.  Perhaps this could be done on the fly based on the detected lane.  For example, take the detected lane and expand it a bit and use that for the active area for subsequent lane detection.  I suspect this works well when it works and is terrible when it gets in a situation where it doesn't work properly.  It needs to be done, though, as it is not reasonable to assume some car and camera geometry as invariant -- it needs to be dynamic.  One reason this is important can be seen in the challenge.  That car in the right lane on the curve interferes with the little isoscolese triangle bound I use.  Camera position and such may be reasonable invariants, but road curve isn't.\n",
    "\n",
    "Another thing it does not do well is track curving roads.  The way I implemented it, it is basically a moving average slope and a moving average nearest point.  From what I have seen this keeps it tangent to the line nearest you fairly well.  This works reasonably well for straight line roads, but it is not as good a model for curved roads.  It would be better to fit a higher order function, spline, etc., to a number of line segment centers in order to better estimate the road curvature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video test_videos_output/solidYellowLeft.mp4\n",
      "[MoviePy] Writing video test_videos_output/solidYellowLeft.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 641/641 [00:06<00:00, 101.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: test_videos_output/solidYellowLeft.mp4 \n",
      "\n",
      "[MoviePy] >>>> Building video test_videos_output/solidWhiteRight.mp4\n",
      "[MoviePy] Writing video test_videos_output/solidWhiteRight.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:01<00:00, 100.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: test_videos_output/solidWhiteRight.mp4 \n",
      "\n",
      "[MoviePy] >>>> Building video test_videos_output/challenge.mp4\n",
      "[MoviePy] Writing video test_videos_output/challenge.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211/211 [00:04<00:00, 50.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: test_videos_output/challenge.mp4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from moviepy.editor import *\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "# Mundane functions\n",
    "def grayscale(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "def canny(img, low_threshold, high_threshold):\n",
    "    return cv2.Canny(img, low_threshold, high_threshold)\n",
    "\n",
    "def gaussian_blur(img, kernel_size):\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    mask = np.zeros_like(img)   \n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  \n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "    cv2.fillPoly(mask, [vertices], ignore_mask_color)\n",
    "    return cv2.bitwise_and(img, mask)\n",
    "\n",
    "def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n",
    "    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "    line_img = np.zeros(img.shape, dtype=np.uint8)\n",
    "    return draw_lines(line_img, lines)\n",
    "   \n",
    "def weighted_img(img, initial_img, α=1.0, β=1., λ=0.):\n",
    "    return cv2.addWeighted(initial_img, α, img, β, λ)\n",
    "\n",
    "def image_list():\n",
    "    return os.listdir('test_images/')\n",
    "\n",
    "def video_list():\n",
    "    return os.listdir('test_videos/')\n",
    "\n",
    "def load_image(image_name):\n",
    "    return mpimg.imread('test_images/' + image_name)\n",
    "\n",
    "def load_video(video_name):\n",
    "    return VideoFileClip('test_videos/' + video_name)\n",
    "\n",
    "def save_image(image, image_name):\n",
    "    plt.imshow(image)  #cmap='gray'\n",
    "    plt.savefig('test_images_output/' + image_name, bbox_inches='tight')\n",
    "\n",
    "def save_video(video, video_name):\n",
    "    video.write_videofile('test_videos_output/' + video_name, audio=False)\n",
    "    \n",
    "def run_image(image_name):\n",
    "    image = load_image(image_name)\n",
    "    processed_image = process_image(image)\n",
    "    save_image(processed_image, image_name)\n",
    "    \n",
    "def run_video(video_name):\n",
    "    video = load_video(video_name)\n",
    "    processed_video = video.fl_image(process_image)\n",
    "    save_video(processed_video, video_name)\n",
    "    \n",
    "    \n",
    "def run_video2(video_name):\n",
    "    clip = load_video(video_name)\n",
    "    frame_count = 0\n",
    "    new_frames = []\n",
    "    for frame in clip.iter_frames():\n",
    "        frame_count += 1\n",
    "        new_frame = process_image(frame)\n",
    "        if (frame_count > 40):\n",
    "            new_frames.append(new_frame)\n",
    "    new_clip = ImageSequenceClip(new_frames, fps = clip.fps)\n",
    "    save_video(new_clip, video_name);\n",
    "    \n",
    "    \n",
    "    \n",
    "# Global vars, yuck    \n",
    "x_near_left_old = 0\n",
    "x_far_left_old = 0\n",
    "x_near_right_old = 0 \n",
    "x_far_right_old = 0\n",
    "\n",
    "slope_left_old = -1.0\n",
    "slope_right_old = 1.0\n",
    "    \n",
    "    \n",
    "    \n",
    "# Functions that matter\n",
    "def draw_lines(img, lines, color=[255, 0, 0], thickness=12, samples = 8):\n",
    "   \n",
    "    global x_near_left_old\n",
    "    global x_far_left_old\n",
    "    global x_near_right_old\n",
    "    global x_far_right_old\n",
    "    \n",
    "    global slope_left_old\n",
    "    global slope_right_old\n",
    "    \n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    \n",
    "    slope_left_sum = 0\n",
    "    slope_left_count = 0\n",
    "    slope_right_sum = 0\n",
    "    slope_right_count = 0\n",
    "    \n",
    "    nearest_left_x = 0\n",
    "    nearest_left_y = 0\n",
    "    nearest_right_x = 0\n",
    "    nearest_right_y = 0\n",
    "        \n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            \n",
    "            # Discount things far away as they are noisy\n",
    "            if ((y1 < int(0.5 * height)) or (y2 < int(0.5 * height))):\n",
    "                continue\n",
    "            \n",
    "            # Slope\n",
    "            m = (y2 - y1) / (x2 - x1)\n",
    "            \n",
    "            # Left - constrain slope and horizontal position\n",
    "            if ((m < -0.5) & (m > -1.0) & (x1 < int( 0.5 * width))):\n",
    "                slope_left_count += 1\n",
    "                slope_left_sum += m\n",
    "                if y1 > nearest_left_y:\n",
    "                    nearest_left_y = y1\n",
    "                    nearest_left_x = x1\n",
    "                if y2 > nearest_left_y:\n",
    "                    nearest_left_y = y2\n",
    "                    nearest_left_x = x2\n",
    "             \n",
    "            # Right - constrain slope and horizontal position\n",
    "            if ((m > 0.5) and (m < 1.0) and (x1 > int(0.5 * width))):\n",
    "                slope_right_count += 1\n",
    "                slope_right_sum += m\n",
    "                if y1 > nearest_right_y:\n",
    "                    nearest_right_y = y1\n",
    "                    nearest_right_x = x1\n",
    "                if y2 > nearest_right_y:\n",
    "                    nearest_right_y = y2\n",
    "                    nearest_right_x = x2\n",
    "             \n",
    "    if slope_left_count > 0:\n",
    "        slope_left_avg = slope_left_sum / slope_left_count\n",
    "        slope_left_avg = 1 / slope_left_avg\n",
    "        \n",
    "        slope_left_avg = (slope_left_avg + (samples - 1) * slope_left_old ) / samples\n",
    "        slope_left_old = slope_left_avg\n",
    "        \n",
    "        x_near = int(nearest_left_x + slope_left_avg * (height - nearest_left_y))\n",
    "        x_far = int(nearest_left_x - slope_left_avg * (nearest_left_y - int(0.65 * height)))\n",
    "        \n",
    "        x_near = int((x_near + (samples - 1) * x_near_left_old) / samples)\n",
    "        x_near_left_old = x_near\n",
    "        x_far = int((x_far + (samples - 1) * x_far_left_old) / samples)\n",
    "        x_far_left_old = x_far\n",
    "        \n",
    "        cv2.line(img, (x_near, height), (x_far, int(0.65 * height)), color, thickness)\n",
    "    else:\n",
    "        cv2.line(img, (x_near_left_old, height), (x_far_left_old, int(0.65 * height)), color, thickness)\n",
    "    \n",
    "    if slope_right_count > 0:\n",
    "        slope_right_avg = slope_right_sum / slope_right_count\n",
    "        slope_right_avg = 1 / slope_right_avg\n",
    "        \n",
    "        slope_right_avg = (slope_right_avg + (samples - 1) * slope_right_old ) / samples\n",
    "        slope_right_old = slope_right_avg\n",
    "        \n",
    "        x_near = int(nearest_right_x + slope_right_avg * (height - nearest_right_y))\n",
    "        x_far = int(nearest_right_x - slope_right_avg * (nearest_right_y - int(0.65 * height)))\n",
    "        \n",
    "        x_near = int((x_near + (samples - 1) * x_near_right_old) / samples)\n",
    "        x_near_right_old = x_near\n",
    "        x_far = int((x_far + (samples - 1) * x_far_right_old) / samples)\n",
    "        x_far_right_old = x_far\n",
    "        \n",
    "        cv2.line(img, (x_near, height), (x_far, int(0.65 * height)), color, thickness)\n",
    "    else:\n",
    "        cv2.line(img, (x_near_right_old, height), (x_far_right_old, int(0.65 * height)), color, thickness)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "    \n",
    "# Main image processing pipeline    \n",
    "def process_image(image):\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    g = grayscale(image)\n",
    "    b = gaussian_blur(g, 15)\n",
    "    c = canny(b, 40, 120)\n",
    "    h = hough_lines(c, 1, 0.01, 10, 25, 30)  \n",
    "    v = np.array([ [int(0.1 * width), height], [int(0.45 * width), int(0.5 * height)], [int(0.55 * width), int(0.5 * height)], [int(0.9 * width), height] ])\n",
    "    m = np.zeros_like(h) \n",
    "    ch = np.dstack( (h, m, m) )\n",
    "    r = region_of_interest(ch, v)\n",
    "    return weighted_img(r, image)\n",
    "    \n",
    "\n",
    "# Videos\n",
    "for video_name in video_list():\n",
    "    run_video2(video_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
